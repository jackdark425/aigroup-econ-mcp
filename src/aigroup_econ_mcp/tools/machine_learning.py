
"""
æœºå™¨å­¦ä¹ é›†æˆæ¨¡å—
æä¾›åŸºäºscikit-learnçš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œç”¨äºç»æµæ•°æ®åˆ†æ
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
from pydantic import BaseModel, Field
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Lasso, Ridge
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')


class MLModelResult(BaseModel):
    """æœºå™¨å­¦ä¹ æ¨¡å‹ç»“æœåŸºç±»"""
    model_type: str = Field(description="æ¨¡å‹ç±»å‹")
    r2_score: float = Field(description="RÂ²å¾—åˆ†")
    mse: float = Field(description="å‡æ–¹è¯¯å·®")
    mae: float = Field(description="å¹³å‡ç»å¯¹è¯¯å·®")
    n_obs: int = Field(description="æ ·æœ¬æ•°é‡")
    feature_names: List[str] = Field(description="ç‰¹å¾åç§°")
    feature_importance: Optional[Dict[str, float]] = Field(default=None, description="ç‰¹å¾é‡è¦æ€§")


class RandomForestResult(MLModelResult):
    """éšæœºæ£®æ—å›å½’ç»“æœ"""
    n_estimators: int = Field(description="æ ‘çš„æ•°é‡")
    max_depth: int = Field(description="æœ€å¤§æ·±åº¦")
    oob_score: Optional[float] = Field(default=None, description="è¢‹å¤–å¾—åˆ†")


class GradientBoostingResult(MLModelResult):
    """æ¢¯åº¦æå‡æ ‘å›å½’ç»“æœ"""
    n_estimators: int = Field(description="æ ‘çš„æ•°é‡")
    learning_rate: float = Field(description="å­¦ä¹ ç‡")
    max_depth: int = Field(description="æœ€å¤§æ·±åº¦")


class RegularizedRegressionResult(MLModelResult):
    """æ­£åˆ™åŒ–å›å½’ç»“æœ"""
    alpha: float = Field(description="æ­£åˆ™åŒ–å¼ºåº¦")
    coefficients: Dict[str, float] = Field(description="å›å½’ç³»æ•°")


class CrossValidationResult(BaseModel):
    """äº¤å‰éªŒè¯ç»“æœ"""
    model_type: str = Field(description="æ¨¡å‹ç±»å‹")
    cv_scores: List[float] = Field(description="äº¤å‰éªŒè¯å¾—åˆ†")
    mean_score: float = Field(description="å¹³å‡å¾—åˆ†")
    std_score: float = Field(description="æ ‡å‡†å·®")
    n_splits: int = Field(description="äº¤å‰éªŒè¯æŠ˜æ•°")


class FeatureImportanceResult(BaseModel):
    """ç‰¹å¾é‡è¦æ€§åˆ†æç»“æœ"""
    feature_importance: Dict[str, float] = Field(description="ç‰¹å¾é‡è¦æ€§åˆ†æ•°")
    sorted_features: List[Tuple[str, float]] = Field(description="æŒ‰é‡è¦æ€§æ’åºçš„ç‰¹å¾")
    top_features: List[str] = Field(description="æœ€é‡è¦çš„ç‰¹å¾")


def random_forest_regression(
    y_data: List[float],
    x_data: List[List[float]],
    feature_names: Optional[List[str]] = None,
    n_estimators: int = 100,
    max_depth: Optional[int] = None,
    random_state: int = 42
) -> RandomForestResult:
    """
    éšæœºæ£®æ—å›å½’
    
    ğŸ“Š åŠŸèƒ½è¯´æ˜ï¼š
    ä½¿ç”¨éšæœºæ£®æ—ç®—æ³•è¿›è¡Œå›å½’åˆ†æï¼Œé€‚ç”¨äºéçº¿æ€§å…³ç³»å’Œå¤æ‚äº¤äº’æ•ˆåº”ã€‚
    
    ğŸ“ˆ ç®—æ³•ç‰¹ç‚¹ï¼š
    - é›†æˆå­¦ä¹ ï¼šå¤šä¸ªå†³ç­–æ ‘çš„ç»„åˆ
    - æŠ—è¿‡æ‹Ÿåˆï¼šé€šè¿‡è¢‹å¤–æ ·æœ¬å’Œç‰¹å¾éšæœºé€‰æ‹©
    - éçº¿æ€§å»ºæ¨¡ï¼šèƒ½å¤Ÿæ•æ‰å¤æ‚çš„éçº¿æ€§å…³ç³»
    - ç‰¹å¾é‡è¦æ€§ï¼šæä¾›ç‰¹å¾é‡è¦æ€§æ’åº
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
    - å¤æ‚éçº¿æ€§å…³ç³»å»ºæ¨¡
    - ç‰¹å¾é‡è¦æ€§åˆ†æ
    - é«˜ç»´æ•°æ®å›å½’
    - ç¨³å¥é¢„æµ‹å»ºæ¨¡
    
    âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
    - è®¡ç®—å¤æ‚åº¦è¾ƒé«˜
    - éœ€è¦è°ƒæ•´è¶…å‚æ•°ï¼ˆn_estimators, max_depthï¼‰
    - å¯¹å¼‚å¸¸å€¼ç›¸å¯¹ç¨³å¥
    
    Args:
        y_data: å› å˜é‡æ•°æ®
        x_data: è‡ªå˜é‡æ•°æ®ï¼ŒäºŒç»´åˆ—è¡¨æ ¼å¼
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        n_estimators: æ ‘çš„æ•°é‡ï¼Œé»˜è®¤100
        max_depth: æœ€å¤§æ·±åº¦ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
        random_state: éšæœºç§å­
    
    Returns:
        RandomForestResult: éšæœºæ£®æ—å›å½’ç»“æœ
    """
    # æ•°æ®éªŒè¯
    if not y_data or not x_data:
        raise ValueError("å› å˜é‡å’Œè‡ªå˜é‡æ•°æ®ä¸èƒ½ä¸ºç©º")
    
    if len(y_data) != len(x_data):
        raise ValueError(f"å› å˜é‡å’Œè‡ªå˜é‡çš„è§‚æµ‹æ•°é‡ä¸ä¸€è‡´: y_data={len(y_data)}, x_data={len(x_data)}")
    
    # å‡†å¤‡æ•°æ®
    X = np.array(x_data)
    y = np.array(y_data)
    
    # ç‰¹å¾åç§°å¤„ç†
    if feature_names is None:
        feature_names = [f"x{i}" for i in range(X.shape[1])]
    elif len(feature_names) != X.shape[1]:
        raise ValueError(f"ç‰¹å¾åç§°æ•°é‡({len(feature_names)})ä¸è‡ªå˜é‡æ•°é‡({X.shape[1]})ä¸åŒ¹é…")
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
    rf_model = RandomForestRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        oob_score=True
    )
    rf_model.fit(X_scaled, y)
    
    # é¢„æµ‹
    y_pred = rf_model.predict(X_scaled)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    r2 = r2_score(y, y_pred)
    mse = mean_squared_error(y, y_pred)
    mae = mean_absolute_error(y, y_pred)
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importance = dict(zip(feature_names, rf_model.feature_importances_))
    
    return RandomForestResult(
        model_type="random_forest",
        r2_score=r2,
        mse=mse,
        mae=mae,
        n_obs=len(y),
        feature_names=feature_names,
        feature_importance=feature_importance,
        n_estimators=n_estimators,
        max_depth=max_depth if max_depth is not None else 0,  # 0è¡¨ç¤ºæ— é™åˆ¶
        oob_score=rf_model.oob_score_ if hasattr(rf_model, 'oob_score_') else None
    )


def gradient_boosting_regression(
    y_data: List[float],
    x_data: List[List[float]],
    feature_names: Optional[List[str]] = None,
    n_estimators: int = 100,
    learning_rate: float = 0.1,
    max_depth: int = 3,
    random_state: int = 42
) -> GradientBoostingResult:
    """
    æ¢¯åº¦æå‡æ ‘å›å½’
    
    ğŸ“Š åŠŸèƒ½è¯´æ˜ï¼š
    ä½¿ç”¨æ¢¯åº¦æå‡ç®—æ³•è¿›è¡Œå›å½’åˆ†æï¼Œé€šè¿‡é€æ­¥ä¼˜åŒ–æ®‹å·®æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚
    
    ğŸ“ˆ ç®—æ³•ç‰¹ç‚¹ï¼š
    - é€æ­¥ä¼˜åŒ–ï¼šé€šè¿‡æ¢¯åº¦ä¸‹é™é€æ­¥æ”¹è¿›æ¨¡å‹
    - é«˜ç²¾åº¦ï¼šé€šå¸¸æ¯”éšæœºæ£®æ—æœ‰æ›´å¥½çš„é¢„æµ‹ç²¾åº¦
    - æ­£åˆ™åŒ–ï¼šé€šè¿‡å­¦ä¹ ç‡å’Œæ ‘æ·±åº¦æ§åˆ¶è¿‡æ‹Ÿåˆ
    - ç‰¹å¾é‡è¦æ€§ï¼šæä¾›ç‰¹å¾é‡è¦æ€§æ’åº
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
    - é«˜ç²¾åº¦é¢„æµ‹éœ€æ±‚
    - ç»“æ„åŒ–æ•°æ®å»ºæ¨¡
    - ç«èµ›å’Œå®é™…åº”ç”¨
    - éœ€è¦ç²¾ç»†è°ƒä¼˜çš„åœºæ™¯
    
    âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
    - å¯¹è¶…å‚æ•°æ•æ„Ÿ
    - è®­ç»ƒæ—¶é—´è¾ƒé•¿
    - å®¹æ˜“è¿‡æ‹Ÿåˆï¼ˆéœ€è¦ä»”ç»†è°ƒå‚ï¼‰
    
    Args:
        y_data: å› å˜é‡æ•°æ®
        x_data: è‡ªå˜é‡æ•°æ®ï¼ŒäºŒç»´åˆ—è¡¨æ ¼å¼
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        n_estimators: æ ‘çš„æ•°é‡ï¼Œé»˜è®¤100
        learning_rate: å­¦ä¹ ç‡ï¼Œé»˜è®¤0.1
        max_depth: æœ€å¤§æ·±åº¦ï¼Œé»˜è®¤3
        random_state: éšæœºç§å­
    
    Returns:
        GradientBoostingResult: æ¢¯åº¦æå‡æ ‘å›å½’ç»“æœ
    """
    # æ•°æ®éªŒè¯
    if not y_data or not x_data:
        raise ValueError("å› å˜é‡å’Œè‡ªå˜é‡æ•°æ®ä¸èƒ½ä¸ºç©º")
    
    if len(y_data) != len(x_data):
        raise ValueError(f"å› å˜é‡å’Œè‡ªå˜é‡çš„è§‚æµ‹æ•°é‡ä¸ä¸€è‡´: y_data={len(y_data)}, x_data={len(x_data)}")
    
    # å‡†å¤‡æ•°æ®
    X = np.array(x_data)
    y = np.array(y_data)
    
    # ç‰¹å¾åç§°å¤„ç†
    if feature_names is None:
        feature_names = [f"x{i}" for i in range(X.shape[1])]
    elif len(feature_names) != X.shape[1]:
        raise ValueError(f"ç‰¹å¾åç§°æ•°é‡({len(feature_names)})ä¸è‡ªå˜é‡æ•°é‡({X.shape[1]})ä¸åŒ¹é…")
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # è®­ç»ƒæ¢¯åº¦æå‡æ ‘æ¨¡å‹
    gb_model = GradientBoostingRegressor(
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        max_depth=max_depth,
        random_state=random_state
    )
    gb_model.fit(X_scaled, y)
    
    # é¢„æµ‹
    y_pred = gb_model.predict(X_scaled)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    r2 = r2_score(y, y_pred)
    mse = mean_squared_error(y, y_pred)
    mae = mean_absolute_error(y, y_pred)
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importance = dict(zip(feature_names, gb_model.feature_importances_))
    
    return GradientBoostingResult(
        model_type="gradient_boosting",
        r2_score=r2,
        mse=mse,
        mae=mae,
        n_obs=len(y),
        feature_names=feature_names,
        feature_importance=feature_importance,
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        max_depth=max_depth
    )


def lasso_regression(
    y_data: List[float],
    x_data: List[List[float]],
    feature_names: Optional[List[str]] = None,
    alpha: float = 1.0,
    random_state: int = 42
) -> RegularizedRegressionResult:
    """
    Lassoå›å½’ï¼ˆL1æ­£åˆ™åŒ–ï¼‰
    
    ğŸ“Š åŠŸèƒ½è¯´æ˜ï¼š
    ä½¿ç”¨L1æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’ï¼Œèƒ½å¤Ÿè¿›è¡Œç‰¹å¾é€‰æ‹©å’Œç¨€ç–å»ºæ¨¡ã€‚
    
    ğŸ“ˆ ç®—æ³•ç‰¹ç‚¹ï¼š
    - ç‰¹å¾é€‰æ‹©ï¼šè‡ªåŠ¨å°†ä¸é‡è¦çš„ç‰¹å¾ç³»æ•°å‹ç¼©ä¸º0
    - ç¨€ç–è§£ï¼šäº§ç”Ÿç¨€ç–çš„ç³»æ•°å‘é‡
    - å¯è§£é‡Šæ€§ï¼šä¿ç•™é‡è¦ç‰¹å¾ï¼Œå»é™¤å†—ä½™ç‰¹å¾
    - å¤„ç†å¤šé‡å…±çº¿æ€§ï¼šå¯¹é«˜åº¦ç›¸å…³çš„ç‰¹å¾è¿›è¡Œé€‰æ‹©
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
    - é«˜ç»´æ•°æ®ç‰¹å¾é€‰æ‹©
    - å¤šé‡å…±çº¿æ€§é—®é¢˜
    - ç¨€ç–å»ºæ¨¡éœ€æ±‚
    - å¯è§£é‡Šæ€§è¦æ±‚é«˜çš„åœºæ™¯
    
    âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
    - å¯¹alphaå‚æ•°æ•æ„Ÿ
    - å¯èƒ½è¿‡åº¦å‹ç¼©é‡è¦ç‰¹å¾
    - éœ€è¦æ•°æ®æ ‡å‡†åŒ–
    
    Args:
        y_data: å› å˜é‡æ•°æ®
        x_data: è‡ªå˜é‡æ•°æ®ï¼ŒäºŒç»´åˆ—è¡¨æ ¼å¼
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        alpha: æ­£åˆ™åŒ–å¼ºåº¦ï¼Œé»˜è®¤1.0
        random_state: éšæœºç§å­
    
    Returns:
        RegularizedRegressionResult: Lassoå›å½’ç»“æœ
    """
    return _regularized_regression(
        y_data, x_data, feature_names, alpha, random_state, "lasso"
    )


def ridge_regression(
    y_data: List[float],
    x_data: List[List[float]],
    feature_names: Optional[List[str]] = None,
    alpha: float = 1.0,
    random_state: int = 42
) -> RegularizedRegressionResult:
    """
    Ridgeå›å½’ï¼ˆL2æ­£åˆ™åŒ–ï¼‰
    
    ğŸ“Š åŠŸèƒ½è¯´æ˜ï¼š
    ä½¿ç”¨L2æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’ï¼Œèƒ½å¤Ÿå¤„ç†å¤šé‡å…±çº¿æ€§é—®é¢˜ã€‚
    
    ğŸ“ˆ ç®—æ³•ç‰¹ç‚¹ï¼š
    - ç¨³å®šæ€§ï¼šå¯¹å¤šé‡å…±çº¿æ€§ç¨³å¥
    - æ”¶ç¼©ç³»æ•°ï¼šå°†æ‰€æœ‰ç³»æ•°å‘0æ”¶ç¼©
    - æ— ç‰¹å¾é€‰æ‹©ï¼šä¿ç•™æ‰€æœ‰ç‰¹å¾
    - æ•°å€¼ç¨³å®šæ€§ï¼šæ”¹å–„çŸ©é˜µæ¡ä»¶æ•°
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
    - å¤šé‡å…±çº¿æ€§é—®é¢˜
    - éœ€è¦ç¨³å®šä¼°è®¡çš„åœºæ™¯
    - æ‰€æœ‰ç‰¹å¾éƒ½å¯èƒ½æœ‰è´¡çŒ®çš„æƒ…å†µ
    - å°æ ·æœ¬é«˜ç»´æ•°æ®
    
    âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
    - ä¸è¿›è¡Œç‰¹å¾é€‰æ‹©
    - å¯¹alphaå‚æ•°æ•æ„Ÿ
    - éœ€è¦æ•°æ®æ ‡å‡†åŒ–
    
    Args:
        y_data: å› å˜é‡æ•°æ®
        x_data: è‡ªå˜é‡æ•°æ®ï¼ŒäºŒç»´åˆ—è¡¨æ ¼å¼
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        alpha: æ­£åˆ™åŒ–å¼ºåº¦ï¼Œé»˜è®¤1.0
        random_state: éšæœºç§å­
    
    Returns:
        RegularizedRegressionResult: Ridgeå›å½’ç»“æœ
    """
    return _regularized_regression(
        y_data, x_data, feature_names, alpha, random_state, "ridge"
    )


def _regularized_regression(
    y_data: List[float],
    x_data: List[List[float]],
    feature_names: Optional[List[str]],
    alpha: float,
    random_state: int,
    model_type: str
) -> RegularizedRegressionResult:
    """æ­£åˆ™åŒ–å›å½’å†…éƒ¨å®ç°"""
    # æ•°æ®éªŒè¯
    if not y_data or not x_data:
        raise ValueError("å› å˜é‡å’Œè‡ªå˜é‡æ•°æ®ä¸èƒ½ä¸ºç©º")
    
    if len(y_data) != len(x_data):
        raise ValueError(f"å› å˜é‡å’Œè‡ªå˜é‡çš„è§‚æµ‹æ•°é‡ä¸ä¸€è‡´: y_data={len(y_data)}, x_data={len(x_data)}")
    
    # å‡†å¤‡æ•°æ®
    X = np.array(x_data)
    y = np.array(y_data)
    
    # ç‰¹å¾åç§°å¤„ç†
    if feature_names is None:
        feature_names = [f"x{i}" for i in range(X.shape[1])]
    elif len(feature_names) != X.shape[1]:
        raise ValueError(f"ç‰¹å¾åç§°æ•°é‡({len(feature_names)})ä¸è‡ªå˜é‡æ•°é‡({X.shape[1]})ä¸åŒ¹é…")
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    y_scaled = (y - np.mean(y)) / np.std(y)  # æ ‡å‡†åŒ–å› å˜é‡
    
    # é€‰æ‹©æ¨¡å‹
    if model_type == "lasso":
        model = Lasso(alpha=alpha, random_state=random_state, max_iter=10000)
    elif model_type == "ridge":
        model = Ridge(alpha=alpha, random_state=random_state)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(X_scaled, y_scaled)
    
    # é¢„æµ‹
    y_pred_scaled = model.predict(X_scaled)
    
    # å°†é¢„æµ‹å€¼è½¬æ¢å›åŸå§‹å°ºåº¦
    y_pred = y_pred_scaled * np.std(y) + np.mean(y)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    r2 = r2_score(y, y_pred)
    mse = mean_squared_error(y, y_pred)
    mae = mean_absolute_error(y, y_pred)
    
    # ç³»æ•°ï¼ˆæ³¨æ„ï¼šç”±äºæ ‡å‡†åŒ–ï¼Œç³»æ•°éœ€è¦é€‚å½“è§£é‡Šï¼‰
    coefficients = dict(zip(feature_names, model.coef_))
    
    return RegularizedRegressionResult(
        model_type=model_type,
        r2_score=r2,
        mse=mse,
        mae=mae,
        n_obs=len(y),
        feature_names=feature_names,
        alpha=alpha,
        coefficients=coefficients
    )


def cross_validation(
    y_data: List[float],
    x_data: List[List[float]],
    model_type: str = "random_forest",
    cv_folds: int = 5,
    scoring: str = "r2",
    **model_params
) -> CrossValidationResult:
    """
    äº¤å‰éªŒè¯
    
    ğŸ“Š åŠŸèƒ½è¯´æ˜ï¼š
    é€šè¿‡äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å®šæ€§ã€‚
    
    ğŸ“ˆ éªŒè¯æ–¹æ³•ï¼š
    - KæŠ˜äº¤å‰éªŒè¯ï¼šå°†æ•°æ®åˆ†ä¸ºKä»½ï¼Œè½®æµä½¿ç”¨K-1ä»½è®­ç»ƒï¼Œ1ä»½æµ‹è¯•
    - ç¨³å®šæ€§è¯„ä¼°ï¼šé€šè¿‡å¤šæ¬¡éªŒè¯è¯„ä¼°æ¨¡å‹ç¨³å®šæ€§
    - æ³›åŒ–èƒ½åŠ›ï¼šè¯„ä¼°æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„è¡¨ç°
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
    - æ¨¡å‹é€‰æ‹©å’Œæ¯”è¾ƒ
    - è¶…å‚æ•°è°ƒä¼˜
    - è¯„ä¼°æ¨¡å‹ç¨³å®šæ€§
    - é˜²æ­¢è¿‡æ‹Ÿåˆ
    
    âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
    - è®¡ç®—æˆæœ¬è¾ƒé«˜
    - éœ€è¦è¶³å¤Ÿçš„æ•°æ®é‡
    - æŠ˜æ•°é€‰æ‹©å½±å“ç»“æœç¨³å®šæ€§
    
    Args:
        y_data: å› å˜é‡æ•°æ®
        x_data: è‡ªå˜é‡æ•°æ®ï¼ŒäºŒç»´åˆ—è¡¨æ ¼å¼
        model_type: æ¨¡å‹ç±»å‹ï¼ˆrandom_forest, gradient_boosting, lasso, ridgeï¼‰
        cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°ï¼Œé»˜è®¤5
        scoring: è¯„åˆ†æŒ‡æ ‡ï¼Œé»˜è®¤"r2"
        **model_params: æ¨¡å‹å‚æ•°
    
    Returns:
        CrossValidationResult: äº¤å‰éªŒè¯ç»“æœ
    """
    # æ•°æ®éªŒè¯
    if not y_data or not x_data:
        raise ValueError("å› å˜é‡å’Œè‡ªå˜é‡æ•°æ®ä¸èƒ½ä¸ºç©º")
    
    if len(y_data) != len(x_data):
        raise ValueError(f"å› å˜é‡å’Œè‡ªå˜é‡çš„è§‚æµ‹æ•°é‡ä¸ä¸€è‡´: y_data={len(y_data)}, x_data={len(x_data)}")
    
    if cv_folds < 2 or cv_folds > len(y_data):
        raise ValueError(f"äº¤å‰éªŒè¯æŠ˜æ•°åº”åœ¨2åˆ°æ ·æœ¬æ•°é‡ä¹‹é—´: cv_folds={cv_folds}, n_obs={len(y_data)}")
    
    # å‡†å¤‡æ•°æ®
    X = np.array(x_data)
    y = np.array(y_data)
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # é€‰æ‹©æ¨¡å‹
    if model_type == "random_forest":
        model = RandomForestRegressor(**model_params)
    elif model_type == "gradient_boosting":
        model = GradientBoostingRegressor(**model_params)
    elif model_type == "lasso":
        model = Lasso(**model_params)
    elif model_type == "ridge":
        model = Ridge(**model_params)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
    
    # æ‰§è¡Œäº¤å‰éªŒè¯
    cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
    cv_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring=scoring)
    
    return CrossValidationResult(
        model_type=model_type,
        cv_scores=cv_scores.tolist(),
        mean_score=np.mean(cv_scores),
        std_score=np.std(cv_scores),
        n_splits=cv_folds
    )


def feature_importance_analysis(
    y_data: List[float],
    x_data: List[List[float]],
    feature_names: Optional[List[str]] = None,
    method: str = "random_forest",
    top_k: int = 5
) -> FeatureImportanceResult:
    """
    ç‰¹å¾é‡è¦æ€§åˆ†æ
    
    ğŸ“Š åŠŸèƒ½è¯´æ˜ï¼š
    åˆ†æå„ä¸ªç‰¹å¾å¯¹é¢„æµ‹ç›®æ ‡çš„é‡è¦æ€§ï¼Œå¸®åŠ©ç†è§£æ•°æ®ä¸­çš„å…³é”®å› ç´ ã€‚
    
    ğŸ“ˆ åˆ†ææ–¹æ³•ï¼š
    - åŸºäºæ¨¡å‹ï¼šä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹è®¡ç®—ç‰¹å¾é‡è¦æ€§
    - æ’åºåˆ†æï¼šæŒ‰é‡è¦æ€§å¯¹ç‰¹å¾è¿›è¡Œæ’åº
    - å…³é”®ç‰¹å¾è¯†åˆ«ï¼šè¯†åˆ«æœ€é‡è¦çš„top-kä¸ªç‰¹å¾
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
    - ç‰¹å¾é€‰æ‹©å’Œé™ç»´
    - æ¨¡å‹å¯è§£é‡Šæ€§åˆ†æ
    - ä¸šåŠ¡æ´å¯Ÿæå–
    - æ•°æ®ç†è§£å¢å¼º
    
    âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
    - ä¸åŒæ–¹æ³•å¯èƒ½ç»™å‡ºä¸åŒçš„é‡è¦æ€§æ’åº
    - é‡è¦æ€§åˆ†æ•°æ˜¯ç›¸å¯¹çš„ï¼Œä¸æ˜¯ç»å¯¹çš„
    - éœ€è¦ç»“åˆä¸šåŠ¡çŸ¥è¯†è§£é‡Šç»“æœ
    
    Args:
        y_data: å› å˜é‡æ•°æ®
        x_data: è‡ªå˜é‡æ•°æ®ï¼ŒäºŒç»´åˆ—è¡¨æ ¼å¼
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        method: åˆ†ææ–¹æ³•ï¼ˆrandom_forest, gradient_boostingï¼‰
        top_k: æœ€é‡è¦çš„ç‰¹å¾æ•°é‡ï¼Œé»˜è®¤5
    
    Returns:
        FeatureImportanceResult: ç‰¹å¾é‡è¦æ€§åˆ†æç»“æœ
    """
    # æ•°æ®éªŒè¯
    if not y_data or not x_data:
        raise ValueError("å› å˜é‡å’Œè‡ªå˜é‡æ•°æ®ä¸èƒ½ä¸ºç©º")
    
    if len(y_data) != len(x_data):
        raise ValueError(f"å› å˜é‡å’Œè‡ªå˜é‡çš„è§‚æµ‹æ•°é‡ä¸ä¸€è‡´: y_data={len(y_data)}, x_data={len(x_data)}")
    
    # å‡†å¤‡æ•°æ®
    X = np.array(x_data)
    y = np.array(y_data)
    
    # ç‰¹å¾åç§°å¤„ç†
    if feature_names is None:
        feature_names = [f"x{i}" for i in range(X.shape[1])]
    elif len(feature_names) != X.shape[1]:
        raise ValueError(f"ç‰¹å¾åç§°æ•°é‡({len(feature_names)})ä¸è‡ªå˜é‡æ•°é‡({X.shape[1]})ä¸åŒ¹é…")
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # é€‰æ‹©æ¨¡å‹å¹¶è®¡ç®—ç‰¹å¾é‡è¦æ€§
    if method == "random_forest":
        model = RandomForestRegressor(n_estimators=100, random_state=42)
    elif method == "gradient_boosting":
        model = GradientBoostingRegressor(n_estimators=100, random_state=42)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç‰¹å¾é‡è¦æ€§åˆ†ææ–¹æ³•: {method}")
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(X_scaled, y)
    
    # è·å–ç‰¹å¾é‡è¦æ€§
    importance_scores = model.feature_importances_
    feature_importance = dict(zip(feature_names, importance_scores))
    
    # æŒ‰é‡è¦æ€§æ’åº
    sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
    
    # è·å–æœ€é‡è¦çš„ç‰¹å¾
    top_features = [feature for feature, score in sorted_features[:top_k]]
    
    return FeatureImportanceResult(
        feature_importance=feature_importance,
        sorted_features=sorted_features,
        top_features=top_features
    )


def compare_ml_models(
    y_data: List[float],
    x_data: List[List[float]],
    feature_names: Optional[List[str]] = None,
    models: List[str] = None
) -> Dict[str, Any]:
    """
    æ¯”è¾ƒå¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹
    
    ğŸ“Š åŠŸèƒ½è¯´æ˜ï¼š
    åŒæ—¶è¿è¡Œå¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹å¹¶æ¯”è¾ƒå®ƒä»¬çš„æ€§èƒ½ï¼Œå¸®åŠ©é€‰æ‹©æœ€ä½³æ¨¡å‹ã€‚
    
    ğŸ“ˆ æ¯”è¾ƒæŒ‡æ ‡ï¼š
    - RÂ²å¾—åˆ†ï¼šæ¨¡å‹è§£é‡Šæ–¹å·®çš„æ¯”ä¾‹
    - å‡æ–¹è¯¯å·®ï¼šé¢„æµ‹è¯¯å·®çš„å¹³æ–¹å¹³å‡
    - å¹³å‡ç»å¯¹è¯¯å·®ï¼šé¢„æµ‹è¯¯å·®çš„ç»å¯¹å¹³å‡
    - ç‰¹å¾é‡è¦æ€§ï¼šæ¨¡å‹è®¤ä¸ºçš„é‡è¦ç‰¹å¾
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
    - æ¨¡å‹é€‰æ‹©å’Œæ¯”è¾ƒ
    - ç®—æ³•æ€§èƒ½è¯„ä¼°
    - é¡¹ç›®åˆå§‹é˜¶æ®µæ¨¡å‹ç­›é€‰
    - åŸºå‡†æ¨¡å‹å»ºç«‹
    
    âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
    - ä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„å‡è®¾å’Œé€‚ç”¨åœºæ™¯
    - éœ€è¦ç»“åˆäº¤å‰éªŒè¯ç»“æœ
    - è€ƒè™‘æ¨¡å‹å¤æ‚åº¦å’Œè®¡ç®—æˆæœ¬
    
    Args:
        y_data: å› å˜é‡æ•°æ®
        x_data: è‡ªå˜é‡æ•°æ®ï¼ŒäºŒç»´åˆ—è¡¨æ ¼å¼
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        models: è¦æ¯”è¾ƒçš„æ¨¡å‹åˆ—è¡¨ï¼Œé»˜è®¤æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹
    
    Returns:
        Dict[str, Any]: æ¨¡å‹æ¯”è¾ƒç»“æœ
    """
    if models is None:
        models = ["random_forest", "gradient_boosting", "lasso", "ridge"]
    
    results = {}
    
    for model_name in models:
        try:
            if model_name == "random_forest":
                result = random_forest_regression(y_data, x_data, feature_names)
            elif model_name == "gradient_boosting":
                result = gradient_boosting_regression(y_data, x_data, feature_names)
            elif model_name == "lasso":
                result = lasso_regression(y_data, x_data, feature_names)
            elif model_name == "ridge":
                result = ridge_regression(y_data, x_data, feature_names)
            else:
                continue
            
            results[model_name] = result.model_dump()
            
        except Exception as e:
            print(f"æ¨¡å‹ {model_name} è¿è¡Œå¤±è´¥: {e}")
            continue
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹ï¼ˆåŸºäºRÂ²å¾—åˆ†ï¼‰
    best_model = None
    best_r2 = -float('inf')
    
    for model_name, result in results.items():
        if result['r2_score'] > best_r2:
            best_r2 = result['r2_score']
            best_model = model_name
    
    return {
        "model_results": results,
        "best_model": best_model,
        "best_r2": best_r2,
        "comparison_summary": {
            "total_models": len(results),
            "successful_models": len(results),
            "best_performing": best_model
        }
    }