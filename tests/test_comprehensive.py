
"""
ç»¼åˆæµ‹è¯•æ–‡ä»¶ - éªŒè¯æ‰€æœ‰è®¡é‡ç»æµå­¦å·¥å…·æ¨¡å—åŠŸèƒ½

æµ‹è¯•è¦†ç›–ï¼š
1. ç»Ÿè®¡åˆ†ææ¨¡å—
2. å›å½’åˆ†ææ¨¡å—  
3. æ—¶é—´åºåˆ—åˆ†ææ¨¡å—
4. é¢æ¿æ•°æ®åˆ†ææ¨¡å—
5. æœºå™¨å­¦ä¹ æ¨¡å—
6. ç¼“å­˜æœºåˆ¶
7. å‚æ•°éªŒè¯ç³»ç»Ÿ
8. æ€§èƒ½ç›‘æ§
9. é›†æˆæµ‹è¯•
10. é”™è¯¯å¤„ç†
11. æ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import sys
import os
import time
import numpy as np
import pandas as pd
from typing import Dict, List, Any
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

# å¯¼å…¥æ‰€æœ‰å·¥å…·æ¨¡å—
from aigroup_econ_mcp.tools.statistics import (
    calculate_descriptive_stats, 
    calculate_correlation_matrix,
    perform_hypothesis_test,
    normality_test
)
from aigroup_econ_mcp.tools.regression import (
    perform_ols_regression,
    calculate_vif,
    run_diagnostic_tests,
    stepwise_regression
)
from aigroup_econ_mcp.tools.time_series import (
    check_stationarity,
    calculate_acf_pacf,
    fit_arima_model,
    find_best_arima_order,
    decompose_time_series,
    var_model,
    vecm_model,
    garch_model,
    state_space_model,
    forecast_var,
    impulse_response_analysis,
    variance_decomposition
)
from aigroup_econ_mcp.tools.panel_data import (
    fixed_effects_model,
    random_effects_model,
    hausman_test,
    panel_unit_root_test,
    compare_panel_models
)
from aigroup_econ_mcp.tools.machine_learning import (
    random_forest_regression,
    gradient_boosting_regression,
    lasso_regression,
    ridge_regression,
    cross_validation,
    feature_importance_analysis,
    compare_ml_models
)
from aigroup_econ_mcp.tools.cache import (
    global_econometric_cache,
    cache_result,
    cache_model
)
from aigroup_econ_mcp.tools.validation import (
    validate_econometric_data,
    validate_model_parameters,
    validate_time_series_data,
    ValidationError
)
from aigroup_econ_mcp.tools.monitoring import (
    global_performance_monitor,
    track_progress,
    enable_memory_monitoring,
    disable_memory_monitoring
)
from aigroup_econ_mcp.tools.base import (
    EconometricTool,
    econometric_tool,
    validate_input
)


class ComprehensiveTester:
    """ç»¼åˆæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = {}
        self.performance_data = {}
        self.error_count = 0
        self.success_count = 0
        
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("=" * 80)
        print("å¼€å§‹è¿è¡Œç»¼åˆæµ‹è¯•")
        print("=" * 80)
        
        # å¯ç”¨æ€§èƒ½ç›‘æ§
        global_performance_monitor.enabled = True
        
        # æµ‹è¯•åˆ—è¡¨
        test_methods = [
            self.test_statistics_module,
            self.test_regression_module,
            self.test_time_series_module,
            self.test_panel_data_module,
            self.test_machine_learning_module,
            self.test_cache_mechanism,
            self.test_validation_system,
            self.test_performance_monitoring,
            self.test_integration_scenarios,
            self.test_error_handling,
            self.test_performance_benchmarks
        ]
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        with track_progress(len(test_methods), "ç»¼åˆæµ‹è¯•è¿›åº¦") as tracker:
            for test_method in test_methods:
                tracker.start_step(test_method.__name__)
                try:
                    test_method()
                    self.success_count += 1
                    print(f"âœ… {test_method.__name__} - é€šè¿‡")
                except Exception as e:
                    self.error_count += 1
                    print(f"âŒ {test_method.__name__} - å¤±è´¥: {e}")
                tracker.complete_step()
                tracker.print_progress()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_test_report()
    
    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("æµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)
        print(f"æ€»æµ‹è¯•æ•°: {len(self.test_results)}")
        print(f"æˆåŠŸ: {self.success_count}")
        print(f"å¤±è´¥: {self.error_count}")
        print(f"æˆåŠŸç‡: {self.success_count / len(self.test_results) * 100:.1f}%")
        
        # æ€§èƒ½ç›‘æ§æ‘˜è¦
        perf_summary = global_performance_monitor.get_summary()
        if perf_summary:
            print(f"\næ€§èƒ½ç›‘æ§æ‘˜è¦:")
            print(f"  ç›‘æ§å‡½æ•°æ•°: {perf_summary['total_functions']}")
            print(f"  æ€»æ‰§è¡Œæ—¶é—´: {perf_summary['total_execution_time']:.2f}s")
            print(f"  å¹³å‡æ‰§è¡Œæ—¶é—´: {perf_summary['average_execution_time']:.2f}s")
            print(f"  æœ€å¤§å†…å­˜ä½¿ç”¨: {perf_summary['max_memory_usage_mb']:.1f}MB")
        
        # ç¼“å­˜ç»Ÿè®¡
        cache_stats = global_econometric_cache.get_cache_info()
        if cache_stats:
            print(f"\nç¼“å­˜ç»Ÿè®¡:")
            print(f"  æ¨¡å‹ç¼“å­˜å¤§å°: {cache_stats['model_cache_size']}")
            print(f"  æ•°æ®ç¼“å­˜å¤§å°: {cache_stats['data_cache_size']}")
            result_cache_stats = cache_stats['result_cache_stats']
            if result_cache_stats:
                print(f"  å…¨å±€å‘½ä¸­ç‡: {result_cache_stats['global_hit_rate']:.2f}")
                print(f"  æ€»å‘½ä¸­æ•°: {result_cache_stats['total_hits']}")
                print(f"  æ€»æœªå‘½ä¸­æ•°: {result_cache_stats['total_misses']}")
    
    def create_sample_data(self, n_samples: int = 100) -> Dict[str, Any]:
        """åˆ›å»ºæ ·æœ¬æ•°æ®"""
        np.random.seed(42)
        
        # å®è§‚ç»æµæ•°æ®
        gdp_growth = np.random.normal(3.0, 0.8, n_samples)
        inflation = np.random.normal(2.0, 0.5, n_samples)
        unemployment = np.random.normal(5.0, 1.2, n_samples)
        interest_rate = np.random.normal(2.5, 0.7, n_samples)
        
        # é‡‘èæ•°æ®
        stock_returns = np.random.normal(0.1, 2.0, n_samples)
        volatility = np.abs(np.random.normal(1.5, 0.5, n_samples))
        
        # é¢æ¿æ•°æ®
        entity_ids = [f"Entity_{i//10}" for i in range(n_samples)]
        time_periods = [f"Period_{i%10}" for i in range(n_samples)]
        
        return {
            "gdp_growth": gdp_growth.tolist(),
            "inflation": inflation.tolist(),
            "unemployment": unemployment.tolist(),
            "interest_rate": interest_rate.tolist(),
            "stock_returns": stock_returns.tolist(),
            "volatility": volatility.tolist(),
            "entity_ids": entity_ids,
            "time_periods": time_periods
        }
    
    def test_statistics_module(self):
        """æµ‹è¯•ç»Ÿè®¡åˆ†ææ¨¡å—"""
        print("\n--- æµ‹è¯•ç»Ÿè®¡åˆ†ææ¨¡å— ---")
        
        data = self.create_sample_data(50)
        
        # æè¿°æ€§ç»Ÿè®¡
        stats = calculate_descriptive_stats(data["gdp_growth"])
        assert stats.count == 50
        assert isinstance(stats.mean, float)
        print(f"âœ… æè¿°æ€§ç»Ÿè®¡ - å‡å€¼: {stats.mean:.3f}, æ ‡å‡†å·®: {stats.std:.3f}")
        
        # ç›¸å…³æ€§åˆ†æ
        corr_data = {
            "GDP": data["gdp_growth"],
            "Inflation": data["inflation"],
            "Unemployment": data["unemployment"]
        }
        corr_result = calculate_correlation_matrix(corr_data)
        assert len(corr_result.correlation_matrix) == 3
        print(f"âœ… ç›¸å…³æ€§åˆ†æ - çŸ©é˜µå¤§å°: {len(corr_result.correlation_matrix)}x{len(corr_result.correlation_matrix)}")
        
        # å‡è®¾æ£€éªŒ
        test_result = perform_hypothesis_test(data["gdp_growth"], data["inflation"])
        assert "statistic" in test_result
        assert "p_value" in test_result
        print(f"âœ… å‡è®¾æ£€éªŒ - ç»Ÿè®¡é‡: {test_result['statistic']:.3f}, på€¼: {test_result['p_value']:.3f}")
        
        # æ­£æ€æ€§æ£€éªŒ
        normality_result = normality_test(data["gdp_growth"])
        assert "shapiro_wilk" in normality_result
        assert "kolmogorov_smirnov" in normality_result
        print(f"âœ… æ­£æ€æ€§æ£€éªŒ - Shapiro-Wilk på€¼: {normality_result['shapiro_wilk']['p_value']:.3f}")
        
        self.test_results["statistics"] = "é€šè¿‡"
    
    def test_regression_module(self):
        """æµ‹è¯•å›å½’åˆ†ææ¨¡å—"""
        print("\n--- æµ‹è¯•å›å½’åˆ†ææ¨¡å— ---")
        
        data = self.create_sample_data(50)
        
        # OLSå›å½’
        y_data = data["gdp_growth"]
        X_data = [[inf, unemp] for inf, unemp in zip(data["inflation"], data["unemployment"])]
        feature_names = ["Inflation", "Unemployment"]
        
        ols_result = perform_ols_regression(y_data, X_data, feature_names)
        assert ols_result.rsquared >= 0
        assert len(ols_result.coefficients) == 3  # å¸¸æ•°é¡¹ + 2ä¸ªç‰¹å¾
        print(f"âœ… OLSå›å½’ - RÂ²: {ols_result.rsquared:.3f}, ç³»æ•°æ•°é‡: {len(ols_result.coefficients)}")
        
        # VIFè®¡ç®—
        vif_values = calculate_vif(X_data, feature_names)
        assert len(vif_values) == 2
        print(f"âœ… VIFè®¡ç®— - Inflation: {vif_values.get('Inflation', 0):.2f}, Unemployment: {vif_values.get('Unemployment', 0):.2f}")
        
        # æ¨¡å‹è¯Šæ–­
        diagnostic_result = run_diagnostic_tests(y_data, X_data)
        assert diagnostic_result.dw_statistic > 0
        print(f"âœ… æ¨¡å‹è¯Šæ–­ - Durbin-Watson: {diagnostic_result.dw_statistic:.3f}")
        
        # é€æ­¥å›å½’
        stepwise_result = stepwise_regression(y_data, X_data, feature_names)
        assert "selected_features" in stepwise_result
        print(f"âœ… é€æ­¥å›å½’ - é€‰æ‹©ç‰¹å¾: {stepwise_result['selected_features']}")
        
        self.test_results["regression"] = "é€šè¿‡"
    
    def test_time_series_module(self):
        """æµ‹è¯•æ—¶é—´åºåˆ—åˆ†ææ¨¡å—"""
        print("\n--- æµ‹è¯•æ—¶é—´åºåˆ—åˆ†ææ¨¡å— ---")
        
        data = self.create_sample_data(100)
        
        # å¹³ç¨³æ€§æ£€éªŒ
        stationarity_result = check_stationarity(data["gdp_growth"])
        assert isinstance(stationarity_result.is_stationary, bool)
        print(f"âœ… å¹³ç¨³æ€§æ£€éªŒ - æ˜¯å¦å¹³ç¨³: {stationarity_result.is_stationary}")
        
        # è‡ªç›¸å…³åˆ†æ
        acf_pacf_result = calculate_acf_pacf(data["gdp_growth"], nlags=10)
        assert len(acf_pacf_result.acf_values) == 11  # åŒ…æ‹¬æ»å0
        print(f"âœ… è‡ªç›¸å…³åˆ†æ - ACFæ»åæ•°: {len(acf_pacf_result.acf_values)}")
        
        # ARIMAæ¨¡å‹
        arima_result = fit_arima_model(data["gdp_growth"], order=(1, 0, 1))
        assert arima_result.aic < float('inf')
        print(f"âœ… ARIMAæ¨¡å‹ - AIC: {arima_result.aic:.2f}")
        
        # VARæ¨¡å‹
        var_data = {
            "GDP": data["gdp_growth"],
            "Inflation": data["inflation"]
        }
        var_result = var_model(var_data, max_lags=2)
        assert var_result.order >= 0
        print(f"âœ… VARæ¨¡å‹ - æœ€ä¼˜æ»åé˜¶æ•°: {var_result.order}, AIC: {var_result.aic:.2f}")
        
        # GARCHæ¨¡å‹
        garch_result = garch_model(data["stock_returns"], order=(1, 1))
        assert garch_result.persistence >= 0
        print(f"âœ… GARCHæ¨¡å‹ - æŒä¹…æ€§: {garch_result.persistence:.3f}")
        
        # çŠ¶æ€ç©ºé—´æ¨¡å‹
        state_space_result = state_space_model(data["gdp_growth"], trend=True)
        assert state_space_result.log_likelihood < float('inf')
        print(f"âœ… çŠ¶æ€ç©ºé—´æ¨¡å‹ - å¯¹æ•°ä¼¼ç„¶: {state_space_result.log_likelihood:.2f}")
        
        self.test_results["time_series"] = "é€šè¿‡"
    
    def test_panel_data_module(self):
        """æµ‹è¯•é¢æ¿æ•°æ®åˆ†ææ¨¡å—"""
        print("\n--- æµ‹è¯•é¢æ¿æ•°æ®åˆ†ææ¨¡å— ---")
        
        data = self.create_sample_data(60)  # 6ä¸ªå®ä½“ï¼Œæ¯ä¸ª10ä¸ªæ—¶æœŸ
        
        # å‡†å¤‡é¢æ¿æ•°æ®
        y_data = data["gdp_growth"]
        X_data = [[inf, unemp] for inf, unemp in zip(data["inflation"], data["unemployment"])]
        entity_ids = data["entity_ids"]
        time_periods = data["time_periods"]
        feature_names = ["Inflation", "Unemployment"]
        
        # å›ºå®šæ•ˆåº”æ¨¡å‹
        fe_result = fixed_effects_model(y_data, X_data, entity_ids, time_periods, feature_names)
        assert fe_result.rsquared >= 0
        print(f"âœ… å›ºå®šæ•ˆåº”æ¨¡å‹ - RÂ²: {fe_result.rsquared:.3f}")
        
        # éšæœºæ•ˆåº”æ¨¡å‹
        re_result = random_effects_model(y_data, X_data, entity_ids, time_periods, feature_names)
        assert re_result.rsquared >= 0
        print(f"âœ… éšæœºæ•ˆåº”æ¨¡å‹ - RÂ²: {re_result.rsquared:.3f}")
        
        # Hausmanæ£€éªŒ
        hausman_result = hausman_test(y_data, X_data, entity_ids, time_periods, feature_names)
        assert hausman_result.significant in [True, False]
        print(f"âœ… Hausmanæ£€éªŒ - æ˜¯å¦æ˜¾è‘—: {hausman_result.significant}")
        
        # é¢æ¿å•ä½æ ¹æ£€éªŒ
        panel_unit_result = panel_unit_root_test(data["gdp_growth"], entity_ids, time_periods)
        assert panel_unit_result.stationary in [True, False]
        print(f"âœ… é¢æ¿å•ä½æ ¹æ£€éªŒ - æ˜¯å¦å¹³ç¨³: {panel_unit_result.stationary}")
        
        # æ¨¡å‹æ¯”è¾ƒ
        comparison_result = compare_panel_models(y_data, X_data, entity_ids, time_periods, feature_names)
        assert "recommendation" in comparison_result
        print(f"âœ… é¢æ¿æ¨¡å‹æ¯”è¾ƒ - æ¨è: {comparison_result['recommendation']}")
        
        self.test_results["panel_data"] = "é€šè¿‡"
    
    def test_machine_learning_module(self):
        """æµ‹è¯•æœºå™¨å­¦ä¹ æ¨¡å—"""
        print("\n--- æµ‹è¯•æœºå™¨å­¦ä¹ æ¨¡å— ---")
        
        data = self.create_sample_data(80)
        
        # å‡†å¤‡æ•°æ®
        y_data = data["gdp_growth"]
        X_data = [[inf, unemp, intr] for inf, unemp, intr in 
                  zip(data["inflation"], data["unemployment"], data["interest_rate"])]
        feature_names = ["Inflation", "Unemployment", "InterestRate"]
        
        # éšæœºæ£®æ—å›å½’
        rf_result = random_forest_regression(y_data, X_data, feature_names, n_estimators=50)
        assert rf_result.r2_score >= -1
        print(f"âœ… éšæœºæ£®æ—å›å½’ - RÂ²: {rf_result.r2_score:.3f}")
        
        # æ¢¯åº¦æå‡æ ‘å›å½’
        gb_result = gradient_boosting_regression(y_data, X_data, feature_names, n_estimators=50)
        assert gb_result.r2_score >= -1
        print(f"âœ… æ¢¯åº¦æå‡æ ‘å›å½’ - RÂ²: {gb_result.r2_score:.3f}")
        
        # Lassoå›å½’
        lasso_result = lasso_regression(y_data, X_data, feature_names, alpha=0.1)
        assert lasso_result.r2_score >= -1
        print(f"âœ… Lassoå›å½’ - RÂ²: {lasso_result.r2_score:.3f}")
        
        # Ridgeå›å½’
        ridge_result = ridge_regression(y_data, X_data, feature_names, alpha=0.1)
        assert ridge_result.r2_score >= -1
        print(f"âœ… Ridgeå›å½’ - RÂ²: {ridge_result.r2_score:.3f}")
        
        # äº¤å‰éªŒè¯
        cv_result = cross_validation(y_data, X_data, model_type="random_forest", cv_folds=5)
        assert len(cv_result.cv_scores) == 5
        print(f"âœ… äº¤å‰éªŒè¯ - å¹³å‡å¾—åˆ†: {cv_result.mean_score:.3f}")
        
        # ç‰¹å¾é‡è¦æ€§åˆ†æ
        feature_importance_result = feature_importance_analysis(y_data, X_data, feature_names)
        assert len(feature_importance_result.top_features) > 0
        print(f"âœ… ç‰¹å¾é‡è¦æ€§åˆ†æ - æœ€é‡è¦ç‰¹å¾: {feature_importance_result.top_features[0]}")
        
        # æ¨¡å‹æ¯”è¾ƒ
        model_comparison = compare_ml_models(y_data, X_data, feature_names)
        assert "best_model" in model_comparison
        print(f"âœ… æ¨¡å‹æ¯”è¾ƒ - æœ€ä½³æ¨¡å‹: {model_comparison['best_model']}")
        
        self.test_results["machine_learning"] = "é€šè¿‡"
    
    def test_cache_mechanism(self):
        """æµ‹è¯•ç¼“å­˜æœºåˆ¶"""
        print("\n--- æµ‹è¯•ç¼“å­˜æœºåˆ¶ ---")
        
        # æ¸…ç©ºç¼“å­˜
        global_econometric_cache.result_cache.clear_all()
        
        # åˆ›å»ºæµ‹è¯•å‡½æ•°
        @cache_result(ttl=60, max_size=10)
        def expensive_calculation(x, y):
            time.sleep(0.1)
            return x + y
        
        # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆåº”è¯¥è®¡ç®—ï¼‰
        start_time = time.time()
        result1 = expensive_calculation(5, 10)
        time1 = time.time() - start_time
        
        # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆåº”è¯¥ä»ç¼“å­˜è·å–ï¼‰
        start_time = time.time()
        result2 = expensive_calculation(5, 10)
        time2 = time.time() - start_time
        
        assert result1 == result2 == 15
        assert time2 < time1  # ç¼“å­˜åº”è¯¥æ›´å¿«
        print(f"âœ… ç¼“å­˜æœºåˆ¶ - ç¬¬ä¸€æ¬¡: {time1:.3f}s, ç¬¬äºŒæ¬¡: {time2:.3f}s, åŠ é€Ÿ: {time1/time2:.1f}x")
        
        # æ£€æŸ¥ç¼“å­˜ç»Ÿè®¡
        cache_stats = global_econometric_cache.result_cache.get_function_cache_stats("expensive_calculation")
        assert cache_stats is not None
        assert cache_stats["hits"] >= 1
        print(f"âœ… ç¼“å­˜ç»Ÿè®¡ - å‘½ä¸­æ•°: {cache_stats['hits']}, æœªå‘½ä¸­æ•°: {cache_stats['misses']}")
        
        self.test_results["cache"] = "é€šè¿‡"
    
    def test_validation_system(self):
        """æµ‹è¯•å‚æ•°éªŒè¯ç³»ç»Ÿ"""
        print("\n--- æµ‹è¯•å‚æ•°éªŒè¯ç³»ç»Ÿ ---")
        
        # æœ‰æ•ˆæ•°æ®éªŒè¯
        valid_data = {
            "GDP": [1.0, 2.0, 3.0, 4.0, 5.0],
            "Inflation": [2.1, 2.3, 2.0, 2.4, 2.2]
        }
        validated_data = validate_econometric_data(valid_data)
        assert len(validated_data) == 2
        print(f"âœ… æœ‰æ•ˆæ•°æ®éªŒè¯ - å˜é‡æ•°: {len(validated_data)}")
        
        # æ— æ•ˆæ•°æ®éªŒè¯ï¼ˆåº”è¯¥æŠ›å‡ºå¼‚å¸¸ï¼‰
        try:
            invalid_data = {"GDP": "not_a_list"}
            validate_econometric_data(invalid_data)
            assert False, "åº”è¯¥æŠ›å‡ºValidationError"
        except ValidationError:
            print("âœ… æ— æ•ˆæ•°æ®éªŒè¯ - æ­£ç¡®æŠ›å‡ºå¼‚å¸¸")
        
        # æ¨¡å‹å‚æ•°éªŒè¯
        valid_params = {
            "n_estimators": 100,
            "max_depth": 10,
            "learning_rate": 0.1,
            "alpha": 1.0
        }
        validated_params = validate_model_parameters(valid_params)
        assert validated_params["n_estimators"] == 100
        print(f"âœ… æ¨¡å‹å‚æ•°éªŒè¯ - éªŒè¯å‚æ•°æ•°: {len(validated_params)}")
        
        # æ—¶é—´åºåˆ—æ•°æ®éªŒè¯
        valid_ts_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]
        validated_ts_data = validate_time_series_data(valid_ts_data, min_length=10)
        assert len(validated_ts_data) == 10
        print(f"âœ… æ—¶é—´åºåˆ—éªŒè¯ - æ•°æ®é•¿åº¦: {len(validated_ts_data)}")
        
        self.test_results["validation"] = "é€šè¿‡"
    
    def test_performance_monitoring(self):
        """æµ‹è¯•æ€§èƒ½ç›‘æ§"""
        print("\n--- æµ‹è¯•æ€§èƒ½ç›‘æ§ ---")
        
        # å¯ç”¨å†…å­˜ç›‘æ§
        memory_monitor = enable_memory_monitoring(check_interval=0.1)
        
        # ç›‘æ§ä¸€ä¸ªè®¡ç®—å¯†é›†å‹å‡½æ•°
        @global_performance_monitor.monitor_function
        def monitored_function(data_size):
            # æ¨¡æ‹Ÿè®¡ç®—å¯†é›†å‹æ“ä½œ
            data = np.random.rand(data_size, data_size)
            result = np.linalg.eig(data)
            return result[0].sum()
        
        # æ‰§è¡Œç›‘æ§çš„å‡½æ•°
        result = monitored_function(100)
        assert isinstance(result, float)
        
        # è·å–æ€§èƒ½ç»Ÿè®¡
        stats = global_performance_monitor.get_function_stats("monitored_function")
        assert stats is not None
        assert stats.execution_time > 0
        print(f"âœ… æ€§èƒ½ç›‘æ§ - æ‰§è¡Œæ—¶é—´: {stats.execution_time:.3f}s, å†…å­˜ä½¿ç”¨: {stats.peak_memory_mb:.1f}MB")
        
        # ç¦ç”¨å†…å­˜ç›‘æ§å¹¶è·å–åˆ†æ
        memory_analysis = disable_memory_monitoring()
        assert "memory_increase_mb" in memory_analysis
        print(f"âœ… å†…å­˜ç›‘æ§ - å†…å­˜å¢é•¿: {memory_analysis['memory_increase_mb']:.1f}MB")
        
        self.test_results["performance_monitoring"] = "é€šè¿‡"
    
    def test_integration_scenarios(self):
        """æµ‹è¯•é›†æˆåœºæ™¯"""
        print("\n--- æµ‹è¯•é›†æˆåœºæ™¯ ---")
        
        data = self.create_sample_data(100)
        
        # åœºæ™¯1: å®è§‚ç»æµåˆ†æç®¡é“
        # æè¿°æ€§ç»Ÿè®¡ -> ç›¸å…³æ€§åˆ†æ -> VARæ¨¡å‹ -> è„‰å†²å“åº”åˆ†æ
        print("ğŸ” é›†æˆåœºæ™¯1: å®è§‚ç»æµåˆ†æç®¡é“")
        
        # 1. æè¿°æ€§ç»Ÿè®¡
        gdp_stats = calculate_descriptive_stats(data["gdp_growth"])
        inflation_stats = calculate_descriptive_stats(data["inflation"])
        
        # 2. ç›¸å…³æ€§åˆ†æ
        macro_data = {
            "GDP": data["gdp_growth"],
            "Inflation": data["inflation"],
            "Unemployment": data["unemployment"]
        }
        corr_result = calculate_correlation_matrix(macro_data)
        
        # 3. VARæ¨¡å‹
        var_result = var_model({
            "GDP": data["gdp_growth"][:50],  # ä½¿ç”¨å‰50ä¸ªç‚¹é¿å…æ•°æ®ä¸è¶³
            "Inflation": data["inflation"][:50]
        }, max_lags=2)
        
        # 4. è„‰å†²å“åº”åˆ†æ
        irf_result = impulse_response_analysis({
            "GDP": data["gdp_growth"][:50],
            "Inflation": data["inflation"][:50]
        }, periods=5, max_lags=2)
        
        assert var_result.order >= 0
        assert "impulse_responses" in irf_result
        print(f"âœ… é›†æˆåœºæ™¯1 - VARæ»åé˜¶æ•°: {var_result.order}, è„‰å†²å“åº”è®¡ç®—å®Œæˆ")
        
        # åœºæ™¯2: æœºå™¨å­¦ä¹ é¢„æµ‹ç®¡é“
        # ç‰¹å¾å·¥ç¨‹ -> æ¨¡å‹è®­ç»ƒ -> äº¤å‰éªŒè¯ -> ç‰¹å¾é‡è¦æ€§åˆ†æ
        print("ğŸ” é›†æˆåœºæ™¯2: æœºå™¨å­¦ä¹ é¢„æµ‹ç®¡é“")
        
        # å‡†å¤‡æ•°æ®
        y_data = data["gdp_growth"]
        X_data = [[inf, unemp, intr] for inf, unemp, intr in 
                  zip(data["inflation"], data["unemployment"], data["interest_rate"])]
        feature_names = ["Inflation", "Unemployment", "InterestRate"]
        
        # 1. æ¨¡å‹è®­ç»ƒ
        rf_result = random_forest_regression(y_data, X_data, feature_names)
        
        # 2. äº¤å‰éªŒè¯
        cv_result = cross_validation(y_data, X_data, model_type="random_forest")
        
        # 3. ç‰¹å¾é‡è¦æ€§
        feature_importance = feature_importance_analysis(y_data, X_data, feature_names)
        
        assert rf_result.r2_score >= -1
        assert cv_result.mean_score >= -1
        assert len(feature_importance.top_features) > 0
        print(f"âœ… é›†æˆåœºæ™¯2 - RÂ²: {rf_result.r2_score:.3f}, äº¤å‰éªŒè¯å¾—åˆ†: {cv_result.mean_score:.3f}")
        
        self.test_results["integration"] = "é€šè¿‡"
    
    def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        print("\n--- æµ‹è¯•é”™è¯¯å¤„ç† ---")
        
        # æµ‹è¯•1: ç©ºæ•°æ®
        try:
            calculate_descriptive_stats([])
            assert False, "åº”è¯¥å¤„ç†ç©ºæ•°æ®é”™è¯¯"
        except Exception as e:
            print(f"âœ… ç©ºæ•°æ®å¤„ç† - {type(e).__name__}: {str(e)}")
        
        # æµ‹è¯•2: æ— æ•ˆå‚æ•°
        try:
            perform_ols_regression([1, 2, 3], [[1], [2]])  # æ•°æ®é•¿åº¦ä¸åŒ¹é…
            assert False, "åº”è¯¥å¤„ç†å‚æ•°é”™è¯¯"
        except Exception as e:
            print(f"âœ… æ— æ•ˆå‚æ•°å¤„ç† - {type(e).__name__}: {str(e)}")
        
        # æµ‹è¯•3: æ•°æ®ä¸è¶³
        try:
            var_model({"GDP": [1, 2, 3]}, max_lags=2)  # æ•°æ®ä¸è¶³
            assert False, "åº”è¯¥å¤„ç†æ•°æ®ä¸è¶³é”™è¯¯"
        except Exception as e:
            print(f"âœ… æ•°æ®ä¸è¶³å¤„ç† - {type(e).__name__}: {str(e)}")
        
        # æµ‹è¯•4: æ— æ•ˆæ¨¡å‹å‚æ•°
        try:
            random_forest_regression([1, 2, 3], [[1], [2], [3]], n_estimators=-1)  # æ— æ•ˆå‚æ•°
            assert False, "åº”è¯¥å¤„ç†æ— æ•ˆæ¨¡å‹å‚æ•°"
        except Exception as e:
            print(f"âœ… æ— æ•ˆæ¨¡å‹å‚æ•°å¤„ç† - {type(e).__name__}: {str(e)}")
        
        self.test_results["error_handling"] = "é€šè¿‡"
    
    def test_performance_benchmarks(self):
        """æµ‹è¯•æ€§èƒ½åŸºå‡†"""
        print("\n--- æµ‹è¯•æ€§èƒ½åŸºå‡† ---")
        
        data = self.create_sample_data(200)  # æ›´å¤§çš„æ•°æ®é›†
        
        # åŸºå‡†1: OLSå›å½’æ€§èƒ½
        print("ğŸ“Š åŸºå‡†1: OLSå›å½’æ€§èƒ½")
        y_data = data["gdp_growth"]
        X_data = [[inf, unemp] for inf, unemp in zip(data["inflation"], data["unemployment"])]
        
        start_time = time.time()
        ols_result = perform_ols_regression(y_data, X_data)
        ols_time = time.time() - start_time
        
        assert ols_result.rsquared >= -1
        print(f"âœ… OLSåŸºå‡† - æ—¶é—´: {ols_time:.3f}s, RÂ²: {ols_result.rsquared:.3f}")
        
        # åŸºå‡†2: éšæœºæ£®æ—æ€§èƒ½
        print("ğŸ“Š åŸºå‡†2: éšæœºæ£®æ—æ€§èƒ½")
        start_time = time.time()
        rf_result = random_forest_regression(y_data, X_data, n_estimators=50)
        rf_time = time.time() - start_time
        
        assert rf_result.r2_score >= -1
        print(f"âœ… éšæœºæ£®æ—åŸºå‡† - æ—¶é—´: {rf_time:.3f}s, RÂ²: {rf_result.r2_score:.3f}")
        
        # åŸºå‡†3: VARæ¨¡å‹æ€§èƒ½
        print("ğŸ“Š åŸºå‡†3: VARæ¨¡å‹æ€§èƒ½")
        var_data = {
            "GDP": data["gdp_growth"][:100],  # ä½¿ç”¨å‰100ä¸ªç‚¹
            "Inflation": data["inflation"][:100],
            "Unemployment": data["unemployment"][:100]
        }
        
        start_time = time.time()
        var_result = var_model(var_data, max_lags=3)
        var_time = time.time() - start_time
        
        assert var_result.aic < float('inf')
        print(f"âœ… VARåŸºå‡† - æ—¶é—´: {var_time:.3f}s, AIC: {var_result.aic:.2f}")
        
        # æ€§èƒ½æ¯”è¾ƒ
        print(f"ğŸ“ˆ æ€§èƒ½æ¯”è¾ƒ:")
        print(f"   - OLSå›å½’: {ols_time:.3f}s")
        print(f"   - éšæœºæ£®æ—: {rf_time:.3f}s")
        print(f"   - VARæ¨¡å‹: {var_time:.3f}s")
        
        self.performance_data = {
            "ols_time": ols_time,
            "rf_time": rf_time,
            "var_time": var_time
        }
        
        self.test_results["performance_benchmarks"] = "é€šè¿‡"


def main():
    """ä¸»å‡½æ•°"""
    tester = ComprehensiveTester()
    tester.run_all_tests()


if __name__ == "__main__":
    main()