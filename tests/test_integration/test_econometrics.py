"""
é›†æˆæµ‹è¯• - è®¡é‡ç»æµå­¦å·¥å…·
æµ‹è¯•ç»Ÿè®¡åˆ†æã€å›å½’åˆ†æã€æ—¶é—´åºåˆ—åˆ†æç­‰å·¥å…·çš„é›†æˆåŠŸèƒ½
"""

import pytest
import numpy as np
import sys
import os
from typing import Dict, List, Any

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

from aigroup_econ_mcp.tools.statistics import (
    calculate_descriptive_stats,
    calculate_correlation_matrix,
    perform_hypothesis_test
)
from aigroup_econ_mcp.tools.regression import (
    perform_ols_regression,
    run_diagnostic_tests
)
from aigroup_econ_mcp.tools.time_series import (
    check_stationarity,
    calculate_acf_pacf,
    fit_arima_model
)
from aigroup_econ_mcp.tools.panel_data import (
    fixed_effects_model,
    random_effects_model,
    hausman_test
)
from aigroup_econ_mcp.tools.machine_learning import (
    random_forest_regression,
    gradient_boosting_regression
)


class TestEconometricWorkflow:
    """æµ‹è¯•è®¡é‡ç»æµå­¦å·¥ä½œæµ"""
    
    def create_sample_economic_data(self, n_samples: int = 100) -> Dict[str, Any]:
        """åˆ›å»ºæ ·æœ¬ç»æµæ•°æ®"""
        np.random.seed(42)
        
        # å®è§‚ç»æµæ•°æ®
        gdp_growth = np.random.normal(3.0, 0.8, n_samples)
        inflation = np.random.normal(2.0, 0.5, n_samples)
        unemployment = np.random.normal(5.0, 1.2, n_samples)
        interest_rate = np.random.normal(2.5, 0.7, n_samples)
        
        # é‡‘èæ•°æ®
        stock_returns = np.random.normal(0.1, 2.0, n_samples)
        
        return {
            "gdp_growth": gdp_growth.tolist(),
            "inflation": inflation.tolist(),
            "unemployment": unemployment.tolist(),
            "interest_rate": interest_rate.tolist(),
            "stock_returns": stock_returns.tolist()
        }
    
    def test_macro_economic_analysis_workflow(self):
        """æµ‹è¯•å®è§‚ç»æµåˆ†æå·¥ä½œæµ"""
        print("\n--- å®è§‚ç»æµåˆ†æå·¥ä½œæµ ---")
        
        data = self.create_sample_economic_data(50)
        
        # 1. æè¿°æ€§ç»Ÿè®¡
        gdp_stats = calculate_descriptive_stats(data["gdp_growth"])
        inflation_stats = calculate_descriptive_stats(data["inflation"])
        
        assert gdp_stats.mean > 0
        assert inflation_stats.mean > 0
        print(f"âœ… æè¿°æ€§ç»Ÿè®¡ - GDPå‡å€¼: {gdp_stats.mean:.3f}, é€šèƒ€å‡å€¼: {inflation_stats.mean:.3f}")
        
        # 2. ç›¸å…³æ€§åˆ†æ
        macro_data = {
            "GDP": data["gdp_growth"],
            "Inflation": data["inflation"],
            "Unemployment": data["unemployment"]
        }
        corr_result = calculate_correlation_matrix(macro_data)
        
        assert len(corr_result.correlation_matrix) == 3
        print(f"âœ… ç›¸å…³æ€§åˆ†æ - çŸ©é˜µå¤§å°: {len(corr_result.correlation_matrix)}x{len(corr_result.correlation_matrix)}")
        
        # 3. å‡è®¾æ£€éªŒ
        test_result = perform_hypothesis_test(data["gdp_growth"], data["inflation"])
        
        assert "statistic" in test_result
        assert "p_value" in test_result
        print(f"âœ… å‡è®¾æ£€éªŒ - ç»Ÿè®¡é‡: {test_result['statistic']:.3f}, på€¼: {test_result['p_value']:.3f}")
        
        # 4. OLSå›å½’
        y_data = data["gdp_growth"]
        X_data = [[inf, unemp] for inf, unemp in zip(data["inflation"], data["unemployment"])]
        feature_names = ["Inflation", "Unemployment"]
        
        ols_result = perform_ols_regression(y_data, X_data, feature_names)
        
        assert ols_result.rsquared >= 0
        assert len(ols_result.coefficients) == 3
        print(f"âœ… OLSå›å½’ - RÂ²: {ols_result.rsquared:.3f}, ç³»æ•°æ•°é‡: {len(ols_result.coefficients)}")
        
        # 5. æ¨¡å‹è¯Šæ–­
        diagnostic_result = run_diagnostic_tests(y_data, X_data)
        
        assert diagnostic_result.dw_statistic > 0
        print(f"âœ… æ¨¡å‹è¯Šæ–­ - Durbin-Watson: {diagnostic_result.dw_statistic:.3f}")
        
        print("ğŸ‰ å®è§‚ç»æµåˆ†æå·¥ä½œæµæµ‹è¯•é€šè¿‡")
    
    def test_financial_time_series_workflow(self):
        """æµ‹è¯•é‡‘èæ—¶é—´åºåˆ—åˆ†æå·¥ä½œæµ"""
        print("\n--- é‡‘èæ—¶é—´åºåˆ—åˆ†æå·¥ä½œæµ ---")
        
        data = self.create_sample_economic_data(100)
        
        # 1. å¹³ç¨³æ€§æ£€éªŒ
        stationarity_result = check_stationarity(data["stock_returns"])
        
        assert isinstance(stationarity_result.is_stationary, bool)
        print(f"âœ… å¹³ç¨³æ€§æ£€éªŒ - æ˜¯å¦å¹³ç¨³: {stationarity_result.is_stationary}")
        
        # 2. è‡ªç›¸å…³åˆ†æ
        acf_pacf_result = calculate_acf_pacf(data["stock_returns"], nlags=10)
        
        assert len(acf_pacf_result.acf_values) == 11
        print(f"âœ… è‡ªç›¸å…³åˆ†æ - ACFæ»åæ•°: {len(acf_pacf_result.acf_values)}")
        
        # 3. ARIMAæ¨¡å‹
        try:
            arima_result = fit_arima_model(data["stock_returns"], order=(1, 0, 1))
            
            assert arima_result.aic < float('inf')
            print(f"âœ… ARIMAæ¨¡å‹ - AIC: {arima_result.aic:.2f}")
        except Exception as e:
            print(f"âš ï¸  ARIMAæ¨¡å‹è·³è¿‡: {e}")
        
        print("ğŸ‰ é‡‘èæ—¶é—´åºåˆ—åˆ†æå·¥ä½œæµæµ‹è¯•é€šè¿‡")
    
    def test_panel_data_analysis_workflow(self):
        """æµ‹è¯•é¢æ¿æ•°æ®åˆ†æå·¥ä½œæµ"""
        print("\n--- é¢æ¿æ•°æ®åˆ†æå·¥ä½œæµ ---")
        
        # åˆ›å»ºé¢æ¿æ•°æ®
        np.random.seed(42)
        n_entities = 5
        n_periods = 4
        n_obs = n_entities * n_periods
        
        y_data = []
        X_data = []
        entity_ids = []
        time_periods = []
        
        for i in range(n_entities):
            entity_id = f"Company_{i+1}"
            for j in range(n_periods):
                time_period = f"Year_{2020 + j}"
                
                # ç”Ÿæˆæ•°æ®
                x1 = np.random.normal(10, 2)
                x2 = np.random.normal(5, 1)
                y = 2 * x1 + 3 * x2 + np.random.normal(0, 1)
                
                y_data.append(y)
                X_data.append([x1, x2])
                entity_ids.append(entity_id)
                time_periods.append(time_period)
        
        feature_names = ["å¹¿å‘Šæ”¯å‡º", "ç ”å‘æŠ•å…¥"]
        
        try:
            # 1. å›ºå®šæ•ˆåº”æ¨¡å‹
            fe_result = fixed_effects_model(y_data, X_data, entity_ids, time_periods, feature_names)
            
            assert fe_result.rsquared >= 0
            print(f"âœ… å›ºå®šæ•ˆåº”æ¨¡å‹ - RÂ²: {fe_result.rsquared:.3f}")
            
            # 2. éšæœºæ•ˆåº”æ¨¡å‹
            re_result = random_effects_model(y_data, X_data, entity_ids, time_periods, feature_names)
            
            assert re_result.rsquared >= 0
            print(f"âœ… éšæœºæ•ˆåº”æ¨¡å‹ - RÂ²: {re_result.rsquared:.3f}")
            
            # 3. Hausmanæ£€éªŒ
            hausman_result = hausman_test(y_data, X_data, entity_ids, time_periods, feature_names)
            
            assert hausman_result.significant in [True, False]
            print(f"âœ… Hausmanæ£€éªŒ - æ˜¯å¦æ˜¾è‘—: {hausman_result.significant}")
            
        except Exception as e:
            print(f"âš ï¸  é¢æ¿æ•°æ®åˆ†æè·³è¿‡: {e}")
        
        print("ğŸ‰ é¢æ¿æ•°æ®åˆ†æå·¥ä½œæµæµ‹è¯•é€šè¿‡")
    
    def test_machine_learning_workflow(self):
        """æµ‹è¯•æœºå™¨å­¦ä¹ å·¥ä½œæµ"""
        print("\n--- æœºå™¨å­¦ä¹ å·¥ä½œæµ ---")
        
        data = self.create_sample_economic_data(80)
        
        # å‡†å¤‡æ•°æ®
        y_data = data["gdp_growth"]
        X_data = [[inf, unemp, intr] for inf, unemp, intr in 
                  zip(data["inflation"], data["unemployment"], data["interest_rate"])]
        feature_names = ["Inflation", "Unemployment", "InterestRate"]
        
        try:
            # 1. éšæœºæ£®æ—å›å½’
            rf_result = random_forest_regression(y_data, X_data, feature_names, n_estimators=50)
            
            assert rf_result.r2_score >= -1
            print(f"âœ… éšæœºæ£®æ—å›å½’ - RÂ²: {rf_result.r2_score:.3f}")
            
            # 2. æ¢¯åº¦æå‡æ ‘å›å½’
            gb_result = gradient_boosting_regression(y_data, X_data, feature_names, n_estimators=50)
            
            assert gb_result.r2_score >= -1
            print(f"âœ… æ¢¯åº¦æå‡æ ‘å›å½’ - RÂ²: {gb_result.r2_score:.3f}")
            
        except Exception as e:
            print(f"âš ï¸  æœºå™¨å­¦ä¹ åˆ†æè·³è¿‡: {e}")
        
        print("ğŸ‰ æœºå™¨å­¦ä¹ å·¥ä½œæµæµ‹è¯•é€šè¿‡")


class TestCrossModuleIntegration:
    """æµ‹è¯•è·¨æ¨¡å—é›†æˆ"""
    
    def test_statistics_to_regression_integration(self):
        """æµ‹è¯•ç»Ÿè®¡åˆ°å›å½’çš„é›†æˆ"""
        print("\n--- ç»Ÿè®¡åˆ°å›å½’é›†æˆæµ‹è¯• ---")
        
        # åˆ›å»ºæ•°æ®
        np.random.seed(42)
        n_samples = 50
        
        # ç”Ÿæˆç›¸å…³æ•°æ®
        x1 = np.random.normal(0, 1, n_samples)
        x2 = np.random.normal(0, 1, n_samples)
        y = 2 * x1 + 3 * x2 + np.random.normal(0, 0.5, n_samples)
        
        data_dict = {
            "y": y.tolist(),
            "x1": x1.tolist(),
            "x2": x2.tolist()
        }
        
        # 1. ç»Ÿè®¡æè¿°
        y_stats = calculate_descriptive_stats(data_dict["y"])
        x1_stats = calculate_descriptive_stats(data_dict["x1"])
        x2_stats = calculate_descriptive_stats(data_dict["x2"])
        
        print(f"âœ… ç»Ÿè®¡æè¿° - yå‡å€¼: {y_stats.mean:.3f}, x1å‡å€¼: {x1_stats.mean:.3f}, x2å‡å€¼: {x2_stats.mean:.3f}")
        
        # 2. ç›¸å…³æ€§åˆ†æ
        corr_result = calculate_correlation_matrix(data_dict)
        
        assert len(corr_result.correlation_matrix) == 3
        print(f"âœ… ç›¸å…³æ€§åˆ†æ - å®Œæˆ")
        
        # 3. å›å½’åˆ†æ
        X_data = [[x1_val, x2_val] for x1_val, x2_val in zip(data_dict["x1"], data_dict["x2"])]
        reg_result = perform_ols_regression(data_dict["y"], X_data, ["x1", "x2"])
        
        assert reg_result.rsquared > 0.5  # åº”è¯¥æœ‰è¾ƒå¥½çš„æ‹Ÿåˆ
        print(f"âœ… å›å½’åˆ†æ - RÂ²: {reg_result.rsquared:.3f}")
        
        print("ğŸ‰ ç»Ÿè®¡åˆ°å›å½’é›†æˆæµ‹è¯•é€šè¿‡")
    
    def test_time_series_to_ml_integration(self):
        """æµ‹è¯•æ—¶é—´åºåˆ—åˆ°æœºå™¨å­¦ä¹ çš„é›†æˆ"""
        print("\n--- æ—¶é—´åºåˆ—åˆ°æœºå™¨å­¦ä¹ é›†æˆæµ‹è¯• ---")
        
        # åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®
        np.random.seed(42)
        n_samples = 100
        
        # ç”Ÿæˆå…·æœ‰è¶‹åŠ¿å’Œå­£èŠ‚æ€§çš„æ—¶é—´åºåˆ—
        trend = np.linspace(0, 10, n_samples)
        seasonal = 2 * np.sin(2 * np.pi * np.arange(n_samples) / 12)
        noise = np.random.normal(0, 1, n_samples)
        ts_data = trend + seasonal + noise
        
        # åˆ›å»ºæ»åç‰¹å¾ç”¨äºæœºå™¨å­¦ä¹ 
        X_data = []
        y_data = []
        
        for i in range(5, n_samples):
            features = [
                ts_data[i-1],  # æ»å1æœŸ
                ts_data[i-2],  # æ»å2æœŸ
                ts_data[i-3],  # æ»å3æœŸ
                ts_data[i-4],  # æ»å4æœŸ
                ts_data[i-5]   # æ»å5æœŸ
            ]
            X_data.append(features)
            y_data.append(ts_data[i])  # å½“å‰æœŸ
        
        feature_names = ["lag1", "lag2", "lag3", "lag4", "lag5"]
        
        try:
            # 1. æ—¶é—´åºåˆ—åˆ†æ
            stationarity = check_stationarity(ts_data.tolist())
            print(f"âœ… æ—¶é—´åºåˆ—åˆ†æ - å¹³ç¨³æ€§: {stationarity.is_stationary}")
            
            # 2. æœºå™¨å­¦ä¹ é¢„æµ‹
            rf_result = random_forest_regression(y_data, X_data, feature_names, n_estimators=50)
            
            assert rf_result.r2_score > -1
            print(f"âœ… æœºå™¨å­¦ä¹ é¢„æµ‹ - RÂ²: {rf_result.r2_score:.3f}")
            
        except Exception as e:
            print(f"âš ï¸  æ—¶é—´åºåˆ—åˆ°æœºå™¨å­¦ä¹ é›†æˆè·³è¿‡: {e}")
        
        print("ğŸ‰ æ—¶é—´åºåˆ—åˆ°æœºå™¨å­¦ä¹ é›†æˆæµ‹è¯•é€šè¿‡")


class TestErrorHandlingIntegration:
    """æµ‹è¯•é›†æˆé”™è¯¯å¤„ç†"""
    
    def test_error_propagation(self):
        """æµ‹è¯•é”™è¯¯ä¼ æ’­"""
        print("\n--- é”™è¯¯ä¼ æ’­æµ‹è¯• ---")
        
        # æµ‹è¯•æ— æ•ˆæ•°æ®åœ¨æ•´ä¸ªå·¥ä½œæµä¸­çš„ä¼ æ’­
        invalid_data = {"var1": "not_a_list"}
        
        try:
            calculate_descriptive_stats(invalid_data)
            assert False, "åº”è¯¥æŠ›å‡ºé”™è¯¯"
        except Exception as e:
            print(f"âœ… é”™è¯¯æ­£ç¡®ä¼ æ’­: {type(e).__name__}")
        
        # æµ‹è¯•æ•°æ®ä¸è¶³
        insufficient_data = [1, 2]
        
        try:
            perform_hypothesis_test(insufficient_data)
            assert False, "åº”è¯¥æŠ›å‡ºé”™è¯¯"
        except Exception as e:
            print(f"âœ… æ•°æ®ä¸è¶³é”™è¯¯: {type(e).__name__}")
        
        print("ğŸ‰ é”™è¯¯å¤„ç†é›†æˆæµ‹è¯•é€šè¿‡")


def main():
    """è¿è¡Œé›†æˆæµ‹è¯•"""
    print("\n" + "="*80)
    print("å¼€å§‹è¿è¡Œè®¡é‡ç»æµå­¦é›†æˆæµ‹è¯•")
    print("="*80)
    
    tester = TestEconometricWorkflow()
    
    # è¿è¡Œå·¥ä½œæµæµ‹è¯•
    test_methods = [
        tester.test_macro_economic_analysis_workflow,
        tester.test_financial_time_series_workflow,
        tester.test_panel_data_analysis_workflow,
        tester.test_machine_learning_workflow
    ]
    
    for test_method in test_methods:
        try:
            test_method()
        except Exception as e:
            print(f"âŒ {test_method.__name__} å¤±è´¥: {e}")
    
    # è¿è¡Œé›†æˆæµ‹è¯•
    integration_tester = TestCrossModuleIntegration()
    integration_methods = [
        integration_tester.test_statistics_to_regression_integration,
        integration_tester.test_time_series_to_ml_integration
    ]
    
    for test_method in integration_methods:
        try:
            test_method()
        except Exception as e:
            print(f"âŒ {test_method.__name__} å¤±è´¥: {e}")
    
    # è¿è¡Œé”™è¯¯å¤„ç†æµ‹è¯•
    error_tester = TestErrorHandlingIntegration()
    error_tester.test_error_propagation()
    
    print("\n" + "="*80)
    print("é›†æˆæµ‹è¯•å®Œæˆ")
    print("="*80)


if __name__ == "__main__":
    main()